{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "H3wh2e4i246P"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install langchain-community pypdf\n",
        "!pip install -qU langchain-huggingface\n",
        "!pip install -U langchain-google-genai\n",
        "!pip install \"unstructured[image]\"\n",
        "!pip install pillow opencv-python\n",
        "!pip install tqdm\n",
        "!pip install ddgs\n",
        "!pip install -U langchain langchain-community langchain-core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pe7ajroFF_-6",
        "outputId": "a65c9dfb-9a86-49c9-830b-bc8352e2e669"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YrxCbtSDkvg"
      },
      "outputs": [],
      "source": [
        "#SmartLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "6l1oFzjwDldl"
      },
      "outputs": [],
      "source": [
        "import os, requests\n",
        "from langchain_community.document_loaders import (\n",
        "    PyPDFLoader,\n",
        "    CSVLoader,\n",
        "    WebBaseLoader,\n",
        ")\n",
        "\n",
        "def smart_loader(source: str):\n",
        "    \"\"\"Load documents from file path or URL intelligently.\"\"\"\n",
        "    all_docs = []\n",
        "\n",
        "    if source.startswith(\"http://\") or source.startswith(\"https://\"):\n",
        "        # Web link\n",
        "        if source.endswith(\".pdf\"):\n",
        "            # Download PDF temporarily\n",
        "            temp_path = \"temp.pdf\"\n",
        "            response = requests.get(source)\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            loader = PyPDFLoader(temp_path)\n",
        "            all_docs.extend(loader.load())\n",
        "            os.remove(temp_path)\n",
        "        elif source.endswith(\".csv\"):\n",
        "            temp_path = \"temp.csv\"\n",
        "            response = requests.get(source)\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                f.write(response.content)\n",
        "            loader = CSVLoader(file_path=temp_path)\n",
        "            all_docs.extend(loader.load())\n",
        "            os.remove(temp_path)\n",
        "        else:\n",
        "            # Generic webpage\n",
        "            loader = WebBaseLoader(source)\n",
        "            all_docs.extend(loader.load())\n",
        "    else:\n",
        "        # Local file\n",
        "        if source.endswith(\".pdf\"):\n",
        "            loader = PyPDFLoader(file_path=source)\n",
        "        elif source.endswith(\".csv\"):\n",
        "            loader = CSVLoader(file_path=source)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported file format: {source}\")\n",
        "        all_docs.extend(loader.load())\n",
        "\n",
        "    return all_docs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fus8nqe7FDIq"
      },
      "source": [
        "Chunking Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "J4sjRlZ7DlgS"
      },
      "outputs": [],
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "def load_and_split_data(source: str, chunk_size: int = 1000, chunk_overlap: int = 400):\n",
        "    \"\"\"\n",
        "    Load documents from a file path or URL using smart_loader,\n",
        "    then split them into manageable text chunks.\n",
        "    \"\"\"\n",
        "    # Load documents (smart detection)\n",
        "    all_docs = smart_loader(source)\n",
        "\n",
        "    # Split into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        add_start_index=True,\n",
        "    )\n",
        "    all_splits = text_splitter.split_documents(all_docs)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(all_docs)} documents, split into {len(all_splits)} chunks.\")\n",
        "    return all_docs, all_splits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SNRbX7XpFcl5"
      },
      "source": [
        "Embedding Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "8o2U104wDliu"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"minishlab/potion-base-8M\")\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Assuming you already have this function from before:\n",
        "# from your_module import load_and_split_data\n",
        "\n",
        "def generate_embeddings_from_source(source: str, embeddings_model=None):\n",
        "    \"\"\"\n",
        "    Loads, splits, and embeds text from a file or URL.\n",
        "    Returns: (documents, splits, vectors)\n",
        "    \"\"\"\n",
        "    # Step 1: Load and split the data\n",
        "    all_docs, all_splits = load_and_split_data(source)\n",
        "\n",
        "    # Step 2: Initialize embedding model if not provided\n",
        "    embeddings = embeddings_model or embedding_model\n",
        "\n",
        "    # Step 3: Generate embeddings\n",
        "    all_vectors = []\n",
        "    for doc in tqdm(all_splits, desc=\"Generating embeddings\"):\n",
        "        vec = embeddings.embed_query(doc.page_content)\n",
        "        all_vectors.append(vec)\n",
        "\n",
        "    print(f\"\\n‚úÖ Loaded {len(all_docs)} documents\")\n",
        "    print(f\"‚úÖ Split into {len(all_splits)} chunks\")\n",
        "    print(f\"‚úÖ Generated {len(all_vectors)} embeddings.\")\n",
        "\n",
        "    return all_docs, all_splits, all_vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "EyX0krfOnelH"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -qU langchain-chroma"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "R1YcFTY3O8VG"
      },
      "outputs": [],
      "source": [
        "import chromadb\n",
        "from langchain_chroma import Chroma\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reuse global embedding model and client\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "embedding_model = HuggingFaceEmbeddings(model_name=\"minishlab/potion-base-8M\")\n",
        "\n",
        "cloud_client = chromadb.CloudClient(\n",
        "    api_key=\"ck-AG66yDL8KK4uwvsq8wtXNjz1MUr4VueutwZisyAq4ThT\",\n",
        "    tenant=\"9682eaae-d806-426c-b9bd-2f0682eec51a\",\n",
        ")\n",
        "global_vector_store = None\n",
        "# üß© Combine everything\n",
        "def add_source_to_vector_db(\n",
        "    source: str,\n",
        "    collection_name: str,\n",
        "    embeddings_model=embedding_model,\n",
        "    client=cloud_client,\n",
        "    batch_size: int = 300\n",
        "):\n",
        "    \"\"\"\n",
        "    Loads, splits, embeds, and uploads data from any source (PDF/local/URL) to Chroma Cloud.\n",
        "    \"\"\"\n",
        "\n",
        "    print(f\"\\nüìÑ Processing source: {source}\")\n",
        "\n",
        "    # Step 1: Generate docs, splits, vectors using your pipeline\n",
        "    docs, splits, vectors = generate_embeddings_from_source(source, embeddings_model)\n",
        "\n",
        "    print(f\"‚úÖ Loaded {len(docs)} documents | Split into {len(splits)} chunks\")\n",
        "\n",
        "    # Step 2: Connect or create Chroma collection\n",
        "    vector_store = Chroma(\n",
        "        client=client,\n",
        "        collection_name=collection_name,\n",
        "        embedding_function=embeddings_model,\n",
        "    )\n",
        "    global global_vector_store\n",
        "    global_vector_store = vector_store\n",
        "\n",
        "    # Step 3: Add chunks in batches\n",
        "    ids = []\n",
        "    for i in tqdm(range(0, len(splits), batch_size), desc=\"Uploading to Chroma\"):\n",
        "        batch = splits[i:i+batch_size]\n",
        "        for doc in batch:\n",
        "            doc.metadata.setdefault(\"source\", source)\n",
        "            doc.metadata.setdefault(\"upload_date\", str(datetime.now()))\n",
        "        try:\n",
        "            batch_ids = vector_store.add_documents(documents=batch)\n",
        "            ids.extend(batch_ids)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ö†Ô∏è Error adding batch {i//batch_size + 1}: {e}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ Successfully uploaded {len(ids)} chunks from {source} ‚Üí {collection_name}\")\n",
        "\n",
        "    return ids, vector_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "BNaCbwi8hsst"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5CPMRcWDqX4u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI # Changed import\n",
        "\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"\"\n",
        "\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") # Changed instantiation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "LMPN8dfP-yne"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "def add_data_to_vector_db(sources: list[str], collection_name: str):\n",
        "    \"\"\"\n",
        "    Takes a list of data sources (PDF paths or URLs) and a collection name,\n",
        "    and adds them to ChromaDB.\n",
        "    \"\"\"\n",
        "    if not collection_name:\n",
        "        return \"‚ùå Please specify a collection name.\"\n",
        "\n",
        "    results = []\n",
        "    for src in sources:\n",
        "        try:\n",
        "            ids, vector_store = add_source_to_vector_db(src, collection_name)\n",
        "            results.append(f\"‚úÖ Added {len(ids)} chunks from {src}\")\n",
        "        except Exception as e:\n",
        "            results.append(f\"‚ùå Failed for {src}: {e}\")\n",
        "    return \"\\n\".join(results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hw98rxHn-xS-"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "import ast\n",
        "\n",
        "# ---- Base ingestion tool ----\n",
        "@tool(\"data_ingestion_workflow\", return_direct=True)\n",
        "def data_ingestion_workflow(\n",
        "    external_sources: list[str] = None,\n",
        "    external_collection: str = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Data ingestion workflow.\n",
        "    Modes:\n",
        "    1. Interactive mode (default)\n",
        "    2. External ingestion mode (when external_sources + external_collection are provided)\n",
        "    \"\"\"\n",
        "\n",
        "    # --------------------------------------\n",
        "    # ‚≠ê MODE 2 ‚Äî EXTERNAL BATCH INGESTION ‚≠ê\n",
        "    # --------------------------------------\n",
        "    if external_sources and external_collection:\n",
        "        print(\"üì• Running in EXTERNAL INGESTION MODE...\")\n",
        "        print(\"Sources received from another tool:\", external_sources)\n",
        "        print(\"Collection name:\", external_collection)\n",
        "\n",
        "        result= add_data_to_vector_db(external_sources, external_collection)\n",
        "\n",
        "        return {\n",
        "            \"message\": \"External ingestion completed.\",\n",
        "            \"result\": result,\n",
        "        }\n",
        "\n",
        "    # --------------------------------------\n",
        "    # ‚≠ê MODE 1 ‚Äî INTERACTIVE MODE \n",
        "    # --------------------------------------\n",
        "    print(\"üß† Starting interactive data ingestion session...\")\n",
        "    print(\"Type 'done' to stop.\\n\")\n",
        "\n",
        "    prompt = \"\"\"\n",
        "    You are a structured input generator for data ingestion.\n",
        "\n",
        "    Your task:\n",
        "    - Read the user query carefully.\n",
        "    - Extract two things:\n",
        "        1. A Python list named `sources` containing all PDF paths or URLs mentioned by the user.\n",
        "        2. A Python string named `collection_name` representing the collection name.\n",
        "\n",
        "    Rules:\n",
        "    - Return only two Python variable declarations in this exact format:\n",
        "        [\"/path/file.pdf\", \"https://arxiv.org/pdf/2307.12945.pdf\"]\n",
        "        \"AI_Papers\"\n",
        "    - Do not include markdown.\n",
        "    \"\"\"\n",
        "\n",
        "    llm = model\n",
        "\n",
        "    while True:\n",
        "        query = input(\"\\nüó£Ô∏è Enter your instruction, give links to files or website link (or 'done' to finish):\\n\")\n",
        "        if query.lower().strip() in [\"done\", \"exit\", \"quit\"]:\n",
        "            print(\"‚úÖ Finished ingestion session.\")\n",
        "            break\n",
        "\n",
        "        # Step 1: Ask LLM to extract structured input\n",
        "        response = llm.invoke(prompt + \"\\nUser query:\\n\" + query)\n",
        "        final_output_text = response.content if hasattr(response, \"content\") else str(response)\n",
        "\n",
        "        print(\"\\n--- LLM Output ---\")\n",
        "        print(final_output_text)\n",
        "\n",
        "        try:\n",
        "            # Step 2: Parse LLM output\n",
        "            lines = [line.strip() for line in final_output_text.splitlines() if line.strip()]\n",
        "            sources = ast.literal_eval(lines[0])\n",
        "            collection_name = ast.literal_eval(lines[1])\n",
        "\n",
        "            print(\"‚úÖ Parsed Sources:\", sources)\n",
        "            print(\"‚úÖ Parsed Collection Name:\", collection_name)\n",
        "\n",
        "            # Step 3: Add data to vector DB\n",
        "            result= add_data_to_vector_db(sources, collection_name)\n",
        "            print(\"\\n--- Tool Result ---\")\n",
        "            print(result)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error parsing or invoking tool: {e}\")\n",
        "            continue\n",
        "\n",
        "    return \"üü¢ Ingestion session completed successfully.\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "qBNoIDIs-J8g"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install ddgs serper playwright beautifulsoup4 lxml aiohttp\n",
        "!pip install python-docx\n",
        "!playwright install\n",
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA78lBBh-KHf"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "from ddgs import DDGS\n",
        "import requests\n",
        "from docx import Document\n",
        "from datetime import datetime\n",
        "\n",
        "SERPER_API_KEY = \"\"\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 1. DUCKDUCKGO TEXT SEARCH\n",
        "# ---------------------------------------------------------\n",
        "def ddg_text_search(query, max_results=15):\n",
        "    results = []\n",
        "    try:\n",
        "        with DDGS() as ddgs:\n",
        "            for r in ddgs.text(query, max_results=max_results):\n",
        "                results.append({\n",
        "                    \"title\": r.get(\"title\"),\n",
        "                    \"snippet\": r.get(\"body\"),\n",
        "                    \"link\": r.get(\"href\"),\n",
        "                    \"source\": \"ddg\"\n",
        "                })\n",
        "    except:\n",
        "        pass\n",
        "    return results\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 2. GOOGLE SERPER TEXT SEARCH (Fallback)\n",
        "# ---------------------------------------------------------\n",
        "def serper_text_search(query, max_results=15):\n",
        "    try:\n",
        "        payload = {\"q\": query, \"num\": max_results}\n",
        "        headers = {\"X-API-KEY\": SERPER_API_KEY, \"Content-Type\": \"application/json\"}\n",
        "\n",
        "        res = requests.post(\"https://google.serper.dev/search\", json=payload, headers=headers)\n",
        "        organic = res.json().get(\"organic\", [])\n",
        "\n",
        "        return [{\n",
        "            \"title\": r.get(\"title\"),\n",
        "            \"snippet\": r.get(\"snippet\"),\n",
        "            \"link\": r.get(\"link\"),\n",
        "            \"source\": \"google-serper\"\n",
        "        } for r in organic]\n",
        "    except:\n",
        "        return []\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 3. RANK RESULTS (GOOGLE > DDG)\n",
        "# ---------------------------------------------------------\n",
        "def rank(results):\n",
        "    _priority = {\"google-serper\": 1, \"ddg\": 2}\n",
        "    return sorted(results, key=lambda x: _priority.get(x[\"source\"], 9))\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 4. UNIFIED SEARCH (TEXT ONLY)\n",
        "# ---------------------------------------------------------\n",
        "def unified_search(query, max_results=15):\n",
        "\n",
        "    # TEXT SEARCH\n",
        "    ddg_text = ddg_text_search(query, max_results)\n",
        "    google_text = serper_text_search(query, max_results)\n",
        "\n",
        "    # Combine & rank\n",
        "    all_text = rank(ddg_text + google_text)[:max_results]\n",
        "\n",
        "    return all_text\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 5. EXPORT LINKS ONLY TO DOCX (NO NUMBERING)\n",
        "# ---------------------------------------------------------\n",
        "def save_links_to_docx(query, results, file_path):\n",
        "    doc = Document()\n",
        "\n",
        "    doc.add_heading(f\"Top Search Links for: {query}\", level=1)\n",
        "    doc.add_paragraph(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
        "\n",
        "    # LINKS ONLY (no numbers, plain list)\n",
        "    doc.add_heading(\"Links\", level=2)\n",
        "\n",
        "    if not results:\n",
        "        doc.add_paragraph(\"No results found.\")\n",
        "    else:\n",
        "        for r in results:\n",
        "            doc.add_paragraph(r)\n",
        "\n",
        "    doc.save(file_path)\n",
        "    return file_path\n",
        "\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# 6. MAIN FUNCTION\n",
        "# ---------------------------------------------------------\n",
        "def unified_search_and_export(query, max_results=15):\n",
        "    # unified_search returns ONLY text results now\n",
        "    text_results = unified_search(query, max_results)\n",
        "    return text_results\n",
        "\n",
        "@tool(\"unified_web_search\", return_direct=False)\n",
        "def unified_web_search_tool(query: str, max_results: int = 15) -> dict:\n",
        "    \"\"\"\n",
        "    Performs unified DuckDuckGo + Google Serper text search.\n",
        "    Returns top links in a structured dict (no saving here).\n",
        "    \"\"\"\n",
        "\n",
        "    # Get search results\n",
        "    text_results = unified_search_and_export(\n",
        "        query=query,\n",
        "        max_results=max_results\n",
        "    )\n",
        "\n",
        "    links = [r[\"link\"] for r in text_results]\n",
        "\n",
        "    return {\n",
        "        \"query\": query,\n",
        "        \"count\": len(links),\n",
        "        \"links\": links\n",
        "    }\n",
        "\n",
        "\n",
        "@tool(\"save_links_to_docx\", return_direct=True)\n",
        "def save_links_to_docx_tool(query: str, links: list, file_path: str) -> str:\n",
        "    \"\"\"\n",
        "    Saves a list of search links to a DOCX file.\n",
        "    \"\"\"\n",
        "    if not links:\n",
        "        return \"‚ùå No links provided to save.\"\n",
        "\n",
        "    path = save_links_to_docx(query, links, file_path)\n",
        "    print(\"The Search Results are saved\")\n",
        "\n",
        "    return f\"üìÑ Links saved successfully to: {path}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "NmRxWnlrRxFc"
      },
      "outputs": [],
      "source": [
        "@tool(\"interactive_search_and_save\", return_direct=True)\n",
        "def interactive_search_and_save() -> str:\n",
        "    \"\"\"\n",
        "    Fully interactive loop:\n",
        "    - Ask the user what they want to search\n",
        "    - Show results\n",
        "    - Ask if they want to save (all or selected)\n",
        "    - Save links to DOCX\n",
        "    - Keep updating until user says 'done'\n",
        "    \"\"\"\n",
        "\n",
        "    docx_path = \"interactive_search_results.docx\"\n",
        "    all_saved_links = []\n",
        "\n",
        "    while True:\n",
        "        # Step 1: Ask for search query\n",
        "        user_query = input(\"\\nüîç Enter your search query (or type 'done' to exit): \").strip()\n",
        "        if user_query.lower() == \"done\":\n",
        "            return {\n",
        "                  \"links\": all_saved_links\n",
        "                    }\n",
        "\n",
        "        # Step 2: Search\n",
        "        result = unified_web_search_tool.invoke({\n",
        "            \"query\": user_query,\n",
        "            \"max_results\": 15\n",
        "        })\n",
        "\n",
        "        print(\"\\n--- Search Results ---\")\n",
        "        print(result)\n",
        "\n",
        "        # Parse links back from text\n",
        "        links = result[\"links\"]\n",
        "\n",
        "        # Step 3: Ask user if they want to save\n",
        "        save_choice = input(\n",
        "            \"\\nüíæ Do you want to save links?\\n\"\n",
        "            \"(all / selected / no): \"\n",
        "        ).strip().lower()\n",
        "\n",
        "        if save_choice == \"no\":\n",
        "            print(\"Skipping save.\")\n",
        "            continue\n",
        "\n",
        "        # Step 4: Save all links\n",
        "        if save_choice == \"all\":\n",
        "            all_saved_links.extend(links)\n",
        "            save_links_to_docx(\"Combined Search Results\", all_saved_links, docx_path)\n",
        "            print(f\"‚úîÔ∏è Saved all {len(all_saved_links)} total links.\")\n",
        "            continue\n",
        "\n",
        "        # Step 5: Save selected links\n",
        "        if save_choice == \"selected\":\n",
        "            print(\"\\nEnter indices of links you want to save (comma-separated):\")\n",
        "            for idx, link in enumerate(links):\n",
        "                print(f\"{idx}. {link}\")\n",
        "\n",
        "            selected_input = input(\"Your selection: \").strip()\n",
        "            try:\n",
        "                indices = [int(x.strip()) for x in selected_input.split(\",\")]\n",
        "                selected_links = [links[i] for i in indices if 0 <= i < len(links)]\n",
        "                all_saved_links.extend(selected_links)\n",
        "                save_links_to_docx(\"Combined Search Results\", all_saved_links, docx_path)\n",
        "                print(f\"‚úîÔ∏è Saved selected links ({len(selected_links)} added).\")\n",
        "            except:\n",
        "                print(\"‚ùå Invalid selection. Skipping.\")\n",
        "\n",
        "        else:\n",
        "            print(\"‚ùå Invalid choice. Skipping.\")\n",
        "\n",
        "    # End\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "eWrhlc-V14Ph"
      },
      "outputs": [],
      "source": [
        "from langchain.tools import tool\n",
        "\n",
        "@tool(\"search_to_ingestion_pipeline\", return_direct=True)\n",
        "def search_to_ingestion_pipeline(\n",
        "    collection_name: str = \"search_results\",\n",
        "    run_rag: bool = True\n",
        "\n",
        ") -> dict:\n",
        "    \"\"\"\n",
        "    Runs the complete workflow:\n",
        "    1. Executes interactive_search_and_save() to let the user search & select links.\n",
        "    2. Takes the returned list of links.\n",
        "    3. Passes those links into data_ingestion_workflow() using external ingestion mode.\n",
        "    4. Returns final ingestion result.\n",
        "\n",
        "    Args:\n",
        "        collection_name (str): Vector DB collection to store search results.\n",
        "\n",
        "    Returns:\n",
        "        dict: {\n",
        "            \"links_ingested\": [...],\n",
        "            \"collection_name\": \"...\",\n",
        "            \"ingestion_result\": ...\n",
        "        }\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: Run the interactive search tool\n",
        "    search_output = interactive_search_and_save.invoke({})\n",
        "\n",
        "    # Safety: ensure correct structure\n",
        "    if not isinstance(search_output, dict) or \"links\" not in search_output:\n",
        "        return {\n",
        "            \"error\": \"search tool did not return expected 'links' list\",\n",
        "            \"raw_output\": search_output\n",
        "        }\n",
        "\n",
        "    links = search_output[\"links\"]\n",
        "\n",
        "    if not links:\n",
        "        return {\n",
        "            \"message\": \"No links selected by user, nothing to ingest.\",\n",
        "            \"links_ingested\": []\n",
        "        }\n",
        "    if run_rag:\n",
        "        # Step 2: Run ingestion\n",
        "        ingestion_result = data_ingestion_workflow.invoke({\n",
        "            \"external_sources\": links,\n",
        "            \"external_collection\": collection_name\n",
        "        })\n",
        "        return {\n",
        "            \"links_ingested\": links,\n",
        "            \"collection_name\": collection_name,\n",
        "            \"ingestion_result\": ingestion_result\n",
        "        }\n",
        "    else:\n",
        "        # Skip ingestion\n",
        "        return {\n",
        "            \"message\": \"RAG ingestion skipped because run_rag=False.\",\n",
        "            \"links_selected\": links,\n",
        "            \"analysis_status\": \"skipped\"\n",
        "        }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "vzK9VLAoSKM7"
      },
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_core.language_models.chat_models import BaseChatModel\n",
        "\n",
        "tools = [data_ingestion_workflow ,search_to_ingestion_pipeline]\n",
        "# If desired, specify custom instructions\n",
        "prompt = (\n",
        "          \"\"\"\n",
        "              You are an orchestrator agent.\n",
        "              Ask the User \"If they want to use Their own local data or Want to search online\n",
        "\n",
        "              If the user wants to upload or provide their own data,\n",
        "              use the tool: data_ingestion_workflow.\n",
        "\n",
        "              If the user wants to search or collect data from the web,\n",
        "              use the tool: search_to_ingestion_pipeline.\n",
        "\n",
        "              Ask for clarification if needed.\n",
        "          \"\"\"\n",
        "\n",
        ")\n",
        "\n",
        "orchestrator  = create_agent(model, tools, system_prompt=prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwrB1v95dDLQ",
        "outputId": "4195bd3e-f1c3-40bb-b627-07d06cd62336"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Data Orchestrator ready. \n",
            "\n",
            "üßë Enter what you want to do? (Write quit to end) --- User: /content/Paper_86-Cardio_Edge_Hardware_Software_Co_design_Implementation.pdf in rest collection\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. latin-1\n",
            "üü¢ Ingestion session (project) completed successfully.\n",
            "0\n",
            "1\n",
            "ordinal not in range(256)\n",
            "post: trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=49043e7d-ad50-4e22-af44-46c311ce9a44; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=8a407048-7b0a-4295-8266-ac6c2e5ab3e3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Running in EXTERNAL INGESTION MODE...\n",
            "Sources received from another tool: ['/content/Paper_86-Cardio_Edge_Hardware_Software_Co_design_Implementation.pdf']\n",
            "Collection name: rest\n",
            "\n",
            "üìÑ Processing source: /content/Paper_86-Cardio_Edge_Hardware_Software_Co_design_Implementation.pdf\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. latin-1\n",
            "üü¢ Ingestion session (project) completed successfully.\n",
            "0\n",
            "1\n",
            "ordinal not in range(256)\n",
            "post: trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=a8a268c4-a99d-49af-9f20-a71e9e46ffdc; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=50a170f8-2d0b-4d28-8f14-c035382ba498; patch: trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=49043e7d-ad50-4e22-af44-46c311ce9a44; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=8a407048-7b0a-4295-8266-ac6c2e5ab3e3\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Loaded 13 documents, split into 99 chunks.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Generating embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99/99 [00:00<00:00, 693.36it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Loaded 13 documents\n",
            "‚úÖ Split into 99 chunks\n",
            "‚úÖ Generated 99 embeddings.\n",
            "‚úÖ Loaded 13 documents | Split into 99 chunks\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Uploading to Chroma: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  3.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Successfully uploaded 99 chunks from /content/Paper_86-Cardio_Edge_Hardware_Software_Co_design_Implementation.pdf ‚Üí rest\n",
            "\n",
            "ü§ñ Agent:\n",
            "{\"message\": \"External ingestion completed.\", \"result\": \"‚úÖ Added 99 chunks from /content/Paper_86-Cardio_Edge_Hardware_Software_Co_design_Implementation.pdf\"}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. latin-1\n",
            "üü¢ Ingestion session (project) completed successfully.\n",
            "0\n",
            "1\n",
            "ordinal not in range(256)\n",
            "patch: trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=50a170f8-2d0b-4d28-8f14-c035382ba498; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9; trace=f5e1822f-41bd-4e2b-8da7-9ee018a4a6b9,id=a8a268c4-a99d-49af-9f20-a71e9e46ffdc\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßë Enter what you want to do? (Write quit to end) --- User: done\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. latin-1\n",
            "üü¢ Ingestion session (project) completed successfully.\n",
            "0\n",
            "1\n",
            "ordinal not in range(256)\n",
            "post: trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=ab8cfdd0-cf03-495a-9ab8-2336737dff7e; trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=6b742c90-301e-497c-b492-36b6560fdad6; trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=4ede9754-f5a6-45f3-9da9-83b2a26fec10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "ü§ñ Agent:\n",
            "Do you want to use your own local data or search online?\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langsmith.client:Failed to batch ingest runs: langsmith.utils.LangSmithError: Failed to POST https://api.smith.langchain.com/runs/batch in LangSmith API. latin-1\n",
            "üü¢ Ingestion session (project) completed successfully.\n",
            "0\n",
            "1\n",
            "ordinal not in range(256)\n",
            "patch: trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=ab8cfdd0-cf03-495a-9ab8-2336737dff7e; trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=6b742c90-301e-497c-b492-36b6560fdad6; trace=ab8cfdd0-cf03-495a-9ab8-2336737dff7e,id=4ede9754-f5a6-45f3-9da9-83b2a26fec10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üßë Enter what you want to do? (Write quit to end) --- User: quit\n",
            "üëã Ending session.\n"
          ]
        }
      ],
      "source": [
        "print(\"üöÄ Data Orchestrator ready. \")\n",
        "\n",
        "while True:\n",
        "    query = input(\"\\nüßë Enter what you want to do? (Write quit to end) --- User: \")\n",
        "\n",
        "    if query.lower() in [\"exit\", \"quit\", \"stop\"]:\n",
        "        print(\"üëã Ending session.\")\n",
        "        break\n",
        "\n",
        "    result = orchestrator.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
        "    })\n",
        "\n",
        "    print(\"\\nü§ñ Agent:\")\n",
        "    print(result[\"messages\"][-1].content)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
